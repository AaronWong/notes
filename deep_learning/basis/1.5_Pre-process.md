# 预处理



## 参考资料

【1】吴恩达，deep_learning.ai，第二周第一课，http://www.ai-start.com/dl2017/html/lesson2-week1.html



## 1 归一化输入

训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：

1. 零均值：x=x-u
2. 归一化方差：x/=𝜎^2（𝜎^2为向量， 它的每个特征都有方差）

![1.5_归一化1](./pic/1.5/1.5_归一化1.png)

> 当它们在非常不同的取值范围内，如其中一个从1到1000，另一个从0到1，这对优化算法非常不利。但是仅将它们设置为均化零值，假设方差为1，就像上一张幻灯片里设定的那样，确保所有特征都在相似范围内，通常可以帮助学习算法运行得更快。

![1.5_归一化2](./pic/1.5/1.5_归一化2.png)



## 2 梯度消失/梯度爆炸

训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。

![1.5_梯度计算](./pic/1.5/1.5_梯度计算.png)

备注：假设 g(z) = z



## 3 权重初始化

谨慎地选择随机初始化参数，梯度消失和梯度爆炸问题，虽然不能彻底解决问题。

n表示神经元的输入特征数量。wi=1/n。

一般情况下，l层上的每个神经元都有n^(l-1)个输入。如果激活函数的输入特征被零均值和标准方差化，方差是1，z也会调整到相似范围，这就没解决问题（梯度消失和爆炸问题）。但它确实降低了梯度消失和爆炸问题，因为它给权重矩阵设置了合理值，你也知道，它不能比1大很多，也不能比1小很多，所以梯度没有爆炸或消失过快。

![1.5_权重初始化](./pic/1.5/1.5_权重初始化.png)

* Relu: np.sqrt(2/n^(l-1))
* tanh: np.sqrt(1/n^(l-1))

设置的权重矩阵既不会增长过快，也不会太快下降到0，从而训练出一个权重或梯度不会增长或消失过快的深度网络。