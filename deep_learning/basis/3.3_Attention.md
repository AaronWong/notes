# Attention

## 参考资料

【1】[真正的完全图解Seq2Seq Attention模型 - 盛源车的文章 - 知乎](https://zhuanlan.zhihu.com/p/40920384)

   

## 1 Seq2Seq Attention 

**seq2seq 是**一个Encoder–Decoder 结构的网络，它的输入**是**一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。

![3.3_seq2seq_attention_0](./pic/3.3/3.3_seq2seq_attention_0.jpg)

### 1.1 公式

![3.3_seq2seq_attention_1](./pic/3.3/3.3_seq2seq_attention_1.jpg)

### 1.2 详细图

![3.3_seq2seq_attention_2](./pic/3.3/3.3_seq2seq_attention_2.jpg)

### 1.3 Attention

![3.3_seq2seq_attention_3](./pic/3.3/3.3_seq2seq_attention_3.jpg)

* dot：

![3.3_seq2seq_attention_4](./pic/3.3/3.3_seq2seq_attention_4.jpg)

* general：

![3.3_seq2seq_attention_5](./pic/3.3/3.3_seq2seq_attention_5.jpg)



## 2 Multihead Attention

